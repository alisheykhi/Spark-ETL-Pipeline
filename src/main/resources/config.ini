[SparkSession]
spark.app.name = SPARK_ETL_PIPELINE
spark.master = yarn
spark.driver.memory=16g
spark.executor.memory=8g
spark.executor.instances=16
spark.executor.cores=4
spark.cleaner.referenceTracking.cleanCheckpoints=true
spark.cleaner.referenceTracking.blocking=true
spark.cleaner.referenceTracking.blocking.shuffle=true
spark.sql.sources.partitionOverwriteMode=dynamic
hive.exec.dynamic.partition=true
hive.exec.dynamic.partition.mode=nonstrict
spark.sql.shuffle.partitions=256
spark.streaming.concurrentJobs=8
spark.sql.streaming.metricsEnabled=true
spark.streaming.stopGracefullyOnShutdown=true
spark.hadoop.fs.hdfs.impl.disable.cache=true
spark.sql.broadcastTimeout=3600
spark.scheduler.mode=FAIR

[Stream1ReadStream]
format=kafka
kafka.bootstrap.servers=kafka1:port, kafka2:port
subscribe=topic_name_for_stream1
startingOffsets=earliest
failOnDataLoss=false
maxOffsetsPerTrigger=5000000
kafka.security.protocol=SASL_PLAINTEXT
kafka.sasl.kerberos.service.name=kafka


[Stream2ReadStream]
format=kafka
kafka.bootstrap.servers=kafka1:port, kafka2:port
subscribe=topic_name_for_stream1
startingOffsets=earliest
failOnDataLoss=false
maxOffsetsPerTrigger=2000000
kafka.security.protocol=SASL_PLAINTEXT
kafka.sasl.kerberos.service.name=kafka

[Stream1Schema]
message.delimiter=\\\|
all.columns.name=column1, column2, column3

[Stream2Schema]
message.delimiter=\\\|
all.columns.name=column1, column2, column3


[WriteStream]
output.mode=append
file.output.mode = overwrite
repartition.count=1
trigger.processingTime=1 minute
option.checkpointLocation= path_to_checkpoint_location
file.out.path= path_to_csv_location for writing output in csv
file.out.format= csv
target.table.columns = column1, column2, column3
target.table = taget_hive_table
